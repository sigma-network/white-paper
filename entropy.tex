\section{Entropy collection}
\label{entropy}

When a validator creates a new account, it is important that they not be able to predict its ticket scores. Otherwise, a validator could enumerate possible key pairs until they eventually find one with high-scoring tickets. To prevent such ``grinding'' attacks, we use a new random seed in each epoch. A validator account created in epoch $i$ cannot submit tickets until epoch $i + 2$, so there is a full round of entropy generation in epoch $i + 1$ before the account becomes eligible.

The random seed for each epoch is derived from the signatures of block creators. In particular, each block creator submits $\operatorname{R}(\mathrm{sk}, h)$ as their entropy contribution, where $R$ is a pseudorandom function, sk is their secret key and $h$ is the height of the block. This computation is done in zero knowledge; the details are described in \autoref{block-creation-circuit}. At the end of each epoch, the bitwise XOR of these entropy submissions becomes the random seed of the following epoch.

Note that this scheme has the properties of a verifiable random function (VRF), although it does not use a typical VRF construction. In particular, the pseudorandom computation is deterministic, and its correctness is publically verifiable since a zero-knowledge proof is provided. Therefore, the only way for a block creator to influence the entropy stream is to skip their block, forfeiting the block reward.

\subsection{Profitability of entropy manipulation}

Consider a validator with a stake of $p$, expressed as a fraction of the money supply that is actively staked. Let $b$ be the number of contiguous blocks at the very end of an epoch which they are eligible to create. By submitting or withholding blocks, the validator can manipulate up to $b$ bits of entropy. In other words, they may choose from among $2^b$ random seeds for the next epoch.

Let $n$ be the number of blocks in an epoch. Let $r_1$ be the number of block rewards which the validator would receive in the next epoch with the ``default'' seed, if no manipulation occurs. Let $r_2, \dots, r_{2^b}$ be the number of block rewards resulting from the alternative seeds which the validator could select, less any block rewards they must sacrifice in order to select those seeds.

Clearly $E[r_1] = n p$. We will compare this to the expected rewards resulting from manipulation, which is
$$ E\left[ \max_{1 \le i \le 2^b}(r_i) \right] $$
To calculate this expected value, we start by applying Fubini's theorem with the linearity of expectation:
\begin{align*}
  E\left[ \max_{1 \le i \le 2^b}(r_i) \right] &= \sum_{j=0}^{n} P\left( \max_{1 \le i \le 2^b}(r_i) > j \right) \\
  &= \sum_{j=0}^{n} P(r_1 > j \text{ or } \dots \text { or } r_{2^b} > j) \\
  \intertext{Applying De Morgan's law, this becomes}
  &= \sum_{j=0}^{n} \left( 1 - P(r_1 \le j \text{ and } \dots \text { and } r_{2^b} \le j) \right) \\
  \intertext{We know $r_1, \dots, r_{2^b}$ are independent, since they are derived from separate random seeds, so we can separate their probabilities:}
  &= \sum_{j=0}^{n} \left( 1 - \prod_{i=0}^{2^b} P(r_i \le j) \right) \\
  \intertext{Next, we express these probabilities in terms of the binomial CDF, $F$. We also group the seeds by $s$, the number of skipped blocks needed to select the seed. Note that $r_i \le j$ if and only if the validator receives at least $j + s$ block rewards in the next epoch, to make up for any skipped blocks.}
  &= \sum_{j=0}^{n} \left( 1 - \prod_{s=0}^{b} \prod_{k=0}^{\binom{b}{s}} F(j + s; n, p) \right) \\
  &= \sum_{j=0}^{n} \left( 1 - \prod_{s=0}^{b} F(j + s; n, p)^{\binom{b}{s}} \right)
\end{align*}
We now have a formula for the validator's expected reward given a particular number of manipulable bits $b$. But since $b$ varies, we must calculate the expected reward for all possible values of $b$, weighted by their probabilities:
$$ \sum_{b=0}^{n} P(b) E\left[ \max_{1 \le i \le 2^b}(r_i) \right] $$
Applying the formula we derived above, along with $P(b) = p^b$, yields the result
$$ \sum_{b=0}^{n} \left( p^b \sum_{j=0}^{n} \left( 1 - \prod_{s=0}^{b} F(j + s; n, p)^{\binom{b}{s}} \right) \right) $$
Finally, to compute the advantage from manipulation, $a$, we simply divide this quantity by the expected rewards with the default seed, which is $n p$.

\begin{wrapfigure}{R}{.41\textwidth}
  \begin{tabularx}{.41\textwidth}{|Y|Y|Y|}
    \hline
    $p$ & $n$ & $a$ \\
    \hline
    \multirow{3}{*}{20\%}
    & 100 & $\le 2.14\%$ \\
    \cline{2-3}
    & 1000 & $\le 0.80\%$ \\
    \cline{2-3}
    & 10000 & $\le 0.26\%$ \\
    \hline
    \multirow{3}{*}{30\%}
    & 100 & $\le 2.79\%$ \\
    \cline{2-3}
    & 1000 & $\le 1.02\%$ \\
    \cline{2-3}
    & 10000 & $\le 0.34\%$ \\
    \hline
    \multirow{3}{*}{40\%}
    & 100 & $\le 3.39\%$ \\
    \cline{2-3}
    & 1000 & $\le 1.23\%$ \\
    \cline{2-3}
    & 10000 & $\le 0.41\%$ \\
    \hline
  \end{tabularx}
  \caption{The expected advantage resulting from manipulation.}
  \label{fig:manipulation}
\end{wrapfigure}

We used a computer program to calculate $a$ for various parameters. The results are shown in \autoref{fig:manipulation}. Note that the $a$ figures are upper bounds, since we used some upper bounds in the program rather than computing exact values. We also assumed that manipulators can perfectly predict their representation in the next epoch, whereas in reality, they can only estimate it based on their registration ticket scores.

As you can see, the advantage from manipulation diminishes as the epoch size increases. This can be explained by the law of large numbers---the variance of a validator's block rewards decreases as the number of blocks increases.

\subsection{Alternative designs}

There are several possible ways to mitigate entropy manipulation. One option is to slash the funds of validators who withhold blocks. This is essentially the idea behind RANDAO \cite{randao}, although RANDAO uses a commit-reveal scheme while we use a VRF. This approach would reduce $E[a]$ by making manipulation more costly, although it would also harm validators who accidentally miss their chance to submit a block.

Another option is to use threshold signatures, such as the BLS scheme which is used by Dfinity \cite{hanke2018dfinity}. In a $(t,n)$-threshold signature scheme, the shares of at least $t$ of $n$ parties are required to create the group signature. As long as $t$ validators are honest and able to communicate with one another, the result should be impossible to manipulate.

Yet another option is to pass VRF outputs through a verifiable delay function (VDF), such as the trapdoor VDF proposed by \cite{wesolowski2018efficient}, and take its output as the next epoch's seed. The idea is that validators would not have sufficient time to compute the seeds which would result from submitting or withholding a block. Under normal circumstances, the network would have moved on before the computation can be done.

While these alternative schemes are academically interesting, they would complicate the protocol, and we consider them unnecessary based on the results in \autoref{fig:manipulation}. Particularly with a large epoch size of 10,000 or more, the potential advantages from manipulation are very small.
