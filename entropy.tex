\section{Entropy collection}
\label{entropy}

When a validator creates a new account, it is important that they not be able to predict its ticket scores. Otherwise, a validator could enumerate possible key pairs until they eventually find one with high-scoring tickets. To prevent such ``grinding'' attacks, we use a new random seed in each epoch. A validator account created in epoch $i$ cannot submit tickets until epoch $i + 2$, so there is a full round of entropy generation in epoch $i + 1$ before the account becomes eligible.

The random seed for each epoch is derived from the signatures of block creators. In particular, each block creator submits $\operatorname{R}(\mathrm{sk}, h)$ as their entropy contribution, where $R$ is a pseudorandom function, sk is their secret key and $h$ is the height of the block. This computation is done in zero knowledge; the details are described in \autoref{block-creation-circuit}. At the end of each epoch, the bitwise XOR of these entropy submissions becomes the random seed of the following epoch.

Note that this scheme has the properties of a verifiable random function (VRF), although it does not use a typical VRF construction. In particular, the pseudorandom computation is deterministic, and its correctness is publically verifiable since a zero-knowledge proof is provided. Therefore, the only way for a block creator to influence the entropy stream is to skip their block, forfeiting the block reward.

\subsection{Profitability of entropy manipulation}

Consider a validator with a stake of $p$, expressed as a fraction of the money supply that is actively staked. Let $b$ be the number of contiguous blocks at the very end of an epoch which they are eligible to create. By submitting or withholding blocks, the validator can manipulate up to $b$ bits of entropy. In other words, they may choose from among $2^b$ random seeds for the next epoch.

Net $n$ be the number of blocks in an epoch. Let $r_1$ be the validator's representation in the next epoch (that is, the number of blocks they will be eligible to create) which would result from the ``default'' seed, if no manipulation occurs. Let $r_2, \dots, r_{2^b}$ be the validator's representation resulting from the alternative seeds which the validator can choose from.

If the validator performs no entropy manipulation, their expected block rewards from the next epoch would simply be $n p$. We will compare this to the expected block rewards from a manipulative validator, who calculates their block rewards for all possible seeds, and choses the seed which results in them receiving the most block rewards. In other words, we want to calculate
$$ E\left[ \max_{1 \le i \le 2^b}(r_i) \right] $$
To calculate this expected value, we start by applying Fubini's theorem with the linearity of expectation:
\begin{align*}
  E\left[ \max_{1 \le i \le 2^b}(r_i) \right] &= \sum_{j=0}^{n} P\left( \max_{1 \le i \le 2^b}(r_i) > j \right) \\
  &= \sum_{j=0}^{n} P(r_1 > j \text{ or } \dots \text { or } r_{2^b} > j) \\
  % Alternate version:
  %\intertext{We now apply the principle of inclusion-exclusion. Since $r_1, \dots, r_{2^b}$ are identically distributed, we use $r_1$ in place of the other variables, giving}
  %&= \sum_{j=0}^{\infty} \sum_{k=1}^{2^b} \left( (-1)^{k+1} P(r_1 > r_1 + j)^k \right) \\
  %\intertext{Substituting in the binomial PDF, we have}
  %&= \sum_{j=0}^{\infty} \sum_{k=1}^{2^b} \left( (-1)^{k+1} \sum_{l=r_1 + j + 1}^{n} \left( \binom{n}{l} p^l (1-p)^{n-l} \right) \right) \\
  %&= \sum_{k=1}^{2^b} \left( (-1)^{k+1} \sum_{j=0}^{\infty} \sum_{l=r_1 + j + 1}^{n} \left( \binom{n}{l} p^l (1-p)^{n-l} \right) \right) \\
  %&= \sum_{k=1}^{2^b} \left( (-1)^{k+1} \sum_{l=r_1 + j + 1}^{n} \left( (j + 1) \binom{n}{l} p^l (1-p)^{n-l} \right) \right) \\
  \intertext{Applying De Morgan's law, this becomes}
  &= \sum_{j=0}^{n} \left( 1 - P(r_1 \le j \text{ and } \dots \text { and } r_{2^b} \le j) \right) \\
  \intertext{Since $r_1, \dots, r_{2^b}$ are independent and identically distributed, we can separate the probabilities and use $r_1$ in place of the other variables:}
  &= \sum_{j=0}^{n} \left( 1 - P(r_1 \le j)^{2^b} \right) \\
  \intertext{Finally, applying the binomial CDF, we have}
  &= \sum_{j=0}^{n} \left( 1 - \left( \sum_{k=0}^{j} \binom{n}{k} p^k (1-p)^{n-k} \right)^{2^b} \right)
\end{align*}
where $n$ is the number of blocks in an epoch.

We now have a formula for the validator's expected reward given a particular number of manipulable bits $b$. But since $b$ varies, we want to calculate the expected reward for all possible values of $b$, weighted by their probabilities:
$$ \sum_{b=0}^{n} P(b) E\left[ \max_{1 \le i \le 2^b}(r_i) \right] $$
Applying the formula we derived above, along with $P(b) = p^b$, yields the result
$$ \sum_{b=0}^{n} \left( p^b \sum_{j=0}^{n} \left( 1 - \left( \sum_{k=0}^{j} \binom{n}{k} p^k (1-p)^{n-k} \right)^{2^b} \right) \right) $$

Finally, to compute the expected advantage from manipulation, $E[a]$, we simply divide this quantity by the expected rewards with no manipulation, which is $n p$.

\begin{wrapfigure}{r}{.41\textwidth}
  \begin{tabularx}{.41\textwidth}{|Y|Y|Y|}
    \hline
    $p$ & $n$ & $E[a]$ \\
    \hline
    \multirow{3}{*}{20\%}
    & 100 & $\le 2.71\%$ \\
    \cline{2-3}
    & 1000 & $\le 0.86\%$ \\
    \cline{2-3}
    & 10000 & $\le 0.27\%$ \\
    \hline
    \multirow{3}{*}{30\%}
    & 100 & $\le 3.46\%$ \\
    \cline{2-3}
    & 1000 & $\le 1.09\%$ \\
    \cline{2-3}
    & 10000 & $\le 0.34\%$ \\
    \hline
    \multirow{3}{*}{40\%}
    & 100 & $\le 4.16\%$ \\
    \cline{2-3}
    & 1000 & $\le 1.31\%$ \\
    \cline{2-3}
    & 10000 & $\le 0.42\%$ \\
    \hline
  \end{tabularx}
  \caption{The expected advantage resulting from manipulation.}
  \label{fig:manipulation}
\end{wrapfigure}

We used a computer program to calculate $E[a]$ for various parameters. The results are shown in \autoref{fig:manipulation}. Note that the $E[a]$ figures are upper bounds, since we did not account for any missed block rewards resulting from withholding blocks. We also assumed that manipulators can perfectly predict their representation in the next epoch, whereas in reality, they can only estimate it based on their registration ticket scores.

As you can see, the advantage from manipulation diminishes as the epoch size increases. This can be explained by the law of large numbers---the variance of a validator's representation decreases as the number of samples (that is, blocks) increases.

\subsection{Alternative designs}

There are several possible ways to mitigate entropy manipulation. One option is to slash the funds of validators who withhold blocks. This is essentially the idea behind RANDAO \cite{randao}, although RANDAO uses a commit-reveal scheme while we use a VRF. This approach would reduce $E[a]$ by making manipulation more costly, although it would also harm validators who accidentally miss their chance to submit a block.

Another option is to use threshold signatures, such as the BLS scheme which is used by Dfinity \cite{hanke2018dfinity}. In a $(t,n)$-threshold signature scheme, the shares of at least $t$ of $n$ parties are required to create the group signature. As long as $t$ validators are honest and able to communicate with one another, the result should be impossible to manipulate.

Yet another option is to pass VRF outputs through a verifiable delay function (VDF), such as the trapdoor VDF proposed by \cite{wesolowski2018efficient}, and take its output as the next epoch's seed. The idea is that validators would not have sufficient time to compute the seeds which would result from submitting or withholding a block. Under normal circumstances, the network would have moved on before the computation can be done.

While these alternative schemes are academically interesting, they would complicate the protocol, and we consider them unnecessary based on the results in \autoref{fig:manipulation}. Particularly with a large epoch size of 10,000 or more, the potential advantages from manipulation are very small.
